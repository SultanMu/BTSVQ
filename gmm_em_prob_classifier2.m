function [posteriors, posteriors_class] = gmm_em_prob_classifier(mix1,mix2,X1,X2,X)

% [PCX7,PCX7_class] = gmm_em_prob_classifier (mix_7_N_recur,mix_7_N_norecur,sM7_recur.codebook, sM7_norecur.codebook, sM7.codebook)

% Finds the posterior probablities of the codebook vector using the parameters of mix1
% and mix2. The parameters of mix1 and mix2 are generated by assuming one gaussian per
% data vector and finding its means and variences using EM algorithm. Then these parameters
% are used to classify a new vector.
% This function uses, dcEMGMM, gmm(from Netlab)

% mix1 & mix2 = generated by dcEMGMM (.....)
% X1 = sM7_recur.codebook
% X2 = sM7_no_recur.codebook
% X = sM_combined.codebook

% psteriors =  matrix of two columns 
%              the posteriors of the vector x interms of X1 and X2

% psteriors_class =  matrix of two columns 
%                  lists 1 for class X1 and 0 for class 2
X = X'; % 25x 66
X1 = X1'; % 11 x 66
X2 = X2'; % 14 x 66
[R C] = size (X);
[r1 c1 ] = size (X1);
[r2 c2 ] = size (X2);

% 66 X 25

pR = ((r1) + (r2)); 
% Compute prior probability of each class 
PC1 = r1/pR; 
PC2 = 1-PC1; 
%load Parameter values obtained after cross validation 
%load lung7_2500 
% 
Mu1 = mean(mix1.centres') ;  % 1x66
Sx1 = mix1.covars; % 1x66 
Sy1 = mix1.covars'; % 1x66
Ppr1 = mix1.priors; % 1x66
%clear Mu Sigma_x Sigma_y Pprior 
%load ParamC2 
Mu2 = mean(mix2.centres'); % 
Sx2 = mix2.covars'; 
Sy2 = mix2.covars'; 
Ppr2 = mix2.priors'; 

%Mu = [Mu1 Mu2]; 

%Sx = [Sx1 Sx2]; 
%Sy = [Sy1 Sy2]; 
%Ppr = [Ppr1 Ppr2]; 
d = 2; 

ee1 = zeros (r1,c1);
for r = 1:r1
    % expected value of x-mu
    ee1(r,:) = (X1(r,:)- Mu1);
end
ee1 = mean (ee1*ee1');

ee2 = zeros (r1,c2)
for r = 1:r2
    % expected value of x-mu
    ee2(r2,:) = (X2(r,:)- Mu2);
end
ee2 = mean (ee2*ee2'); % 1 x 66

pXC1 = zeros (R,C);
for r = 1:R
    %for j = 1:length(Mu1(:,1)) 
	pXC1(r,:) =  exp(-1/2* ((X(r,:)- Mu1)'/ee1)*(X(r,:)-Mu1)); 
    %end 
pD1(r) = pXC1(r,:)*Ppr1'; 
end 
pXC2 = zeros (R,C);
for r = 1:R
    %for j = 1:length(Mu1(:,1)) 
	pXC2(r,:) = 1/((2*pi)^(d/2)*ee2.^(1/2))  *  exp(-1/2*  (  ((X(r,:)- Mu2)'/ee2)*(X(r,:)-Mu2))); 
    %end 
pD1(r) = pXC2(r,:)*Ppr2'; 
end 

% Compute posterior probabilities using Bayes Rule 
for ii = 1:R 
	PC1X(ii) = pD1(ii)*PC1/(pD1(ii)*PC1+pD2(ii)*PC2); 
end 
PC2X = 1-PC1X; 
PCX = [PC1X',PC2X']; 
posteriors = PCX;

% Classify 
num = 0; 
for j = 1:N 
	if PC1X(j) < 0.5 
		PC1X(j) = 0; 
		PC2X(j) = 1; 
		num = num + 1; 
	else 
		PC1X(j) = 1; 
		PC2X(j) = 0; 
	end 
end 
PCX_class = [PC1X', PC2X'];
posteriors_class = PCX_class;
%pause
%Per1 = num/N*100 
% Plot results 
%for ii = 1:length(Mu) 
%makeEllipse(Mu(ii,1),Mu(ii,2),2*Sx(ii),2*Sy(ii)) 
%hold on 
%end 
%plot(X(:,1),X(:,2),'+ k') 
%xlabel('x','fontsize',16) 
%ylabel('y','fontsize',16) 
%print -depsc ClassTest.eps 


