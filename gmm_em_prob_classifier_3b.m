function [posteriors, posteriors_class] = gmm_em_prob_classifier_3b(K1,P1,K2,P2,sM1,sM2,X)

% [PCX7,PCX7_class] = gmm_em_prob_classifier (mix_7_N_recur,mix_7_N_norecur,sM7_recur.codebook, sM7_norecur.codebook, sM7.codebook)

% Finds the posterior probablities of the codebook vector using the parameters of mix1
% and mix2. The parameters of mix1 and mix2 are generated by assuming one gaussian per
% data vector and finding its means and variences using EM algorithm. Then these parameters
% are used to classify a new vector.
% This function uses, dcEMGMM, gmm(from Netlab)

% mix1 & mix2 = generated by dcEMGMM (.....)
% X1 = sM7_recur.codebook
% X2 = sM7_no_recur.codebook
% X = sM_combined.codebook

% psteriors =  matrix of two columns 
%              the posteriors of the vector x interms of X1 and X2

% psteriors_class =  matrix of two columns 
%                  lists 1 for class X1 and 0 for class 2

[N NN]= size(X); 
R = (length(sM1.codebook) + length(sM2.codebook)); 
% Compute prior probability of each class 
PC1 = length(sM1.codebook)/R; 
%PC1 = .5; %as both classes are of same size
PC2 = 1-PC1; 



%load Parameter values obtained after cross validation 
%load lung7_2500 
%Mu1 = X1; %mean(X1'); 
%Sx1 = K1; %mean (K1'); 
%Sy1 = K1; 
Ppr1 = P1;
%[N1 d1] = size (X1);
%clear Mu Sigma_x Sigma_y Pprior 
%load ParamC2 
%Mu2 = X2; %mean (X2'); 
%Sx2 = K2; %mean (K2'); 
%Sy2 = K2; 
Ppr2 = P2; 
%[N2 d2] = size (X2);
%X = X; %mean (X');
%[N D] = size (X);
%Mu = [Mu1 Mu2]; 
%Sx = [Sx1 Sx2]; 
%Sy = [Sy1 Sy2]; 
%Ppr = [Ppr1 Ppr2]; 
%d1 = ;
%pXC1 = zeros(length(X),length(Mu1));
%pD1 = zeros (length(X));
%for n = 1:N 
%	for j = 1:N1 
	%pXC1(n,j) = 1/ ((2*pi)^(2/2)*Sx1^2)  *   exp(-1/2 * (((X(n,:)-Mu1(j,:))/Sx1(j,:))^2)); 
%    cov = det(mean ((X(n,:)-Mu1(j,:))*(X(n,:)-Mu1(j,:))'));
%    pXC1(n,j) = 1/ ((2*pi)^(2/2)*cov .^1/2)  *   exp(-1/2 * (    ((X(n,:)-Mu1(j,:))/cov) * ( ((X(n,:)-Mu1(j,:)))') )); 
%    end 
	%pD1(n) = pXC1(n,:)*Ppr1'; 
  [pD1,Pdm1,pmd1] = som_probability_gmm_2(X, sM1, K1, P1);
    %end 

%for n = 1:N
%	for j = 1:N2 
 %	cov = det(mean ((X(n,:)-Mu2(j,:))*(X(n,:)-Mu2(j,:))'));
  %  pXC2(n,j) = 1/ ((2*pi)^(2/2)*cov .^1/2)  *   exp(-1/2 * ( ((X(n,:)-Mu2(j,:))/cov)*(((X(n,:)-Mu2(j,:)))' ))); 
  %	end 
%	pD2(n) = pXC2(n,:)*Ppr2'; 
%end 
 [pD2,Pdm2,pmd2] = som_probability_gmm_2(X, sM2, K2, P2);
% Compute posterior probabilities using Bayes Rule 
for ii = 1:N 
	PC1X(ii) = pD1(ii)*PC1/(pD1(ii)*PC1+pD2(ii)*PC2); 
end 
for ii = 1:N 
	PC2X(ii) = pD2(ii)*PC2/(pD1(ii)*PC1+pD2(ii)*PC2); 
end 
%PC2X = 1-PC1X; 
PCX = [PC1X',PC2X']; 
posteriors = PCX;

% Classify 
num = 0; 
for j = 1:N 
	if PC1X(j) < 0.5 
		PC1X(j) = 0; 
		PC2X(j) = 1; 
		num = num + 1; 
	else 
		PC1X(j) = 1; 
		PC2X(j) = 0; 
	end 
end 
PCX_classs = [PC1X', PC2X'];
posteriors_class= PCX_classs;
%pause
%Per1 = num/N*100 
% Plot results 
%for ii = 1:length(Mu) 
%makeEllipse(Mu(ii,1),Mu(ii,2),2*Sx(ii),2*Sy(ii)) 
%hold on 
%end 
%plot(X(:,1),X(:,2),'+ k') 
%xlabel('x','fontsize',16) 
%ylabel('y','fontsize',16) 
%print -depsc ClassTest.eps 



